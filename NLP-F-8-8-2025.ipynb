{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyZJxLETD6DVeThkC4yPYC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharadwaj103/NLP_1610/blob/main/NLP_F_12_9_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksptljL7Ht-A",
        "outputId": "fb6cf92a-b935-475a-ea99-ea5e666ff3a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 rows of the sample resumes:\n",
            "                                         Resume_Text\n",
            "0  I am a software engineer with 5+ years of expe...\n",
            "1  Data Scientist with expertise in Python, R, an...\n",
            "2  A marketing specialist with a background in di...\n",
            "\n",
            "Checking for noisy characters...\n",
            "\n",
            " present: True\n",
            "• present: True\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample resume data create chesthunna\n",
        "data = {'Resume_Text': [\n",
        "    \"I am a software engineer with 5+ years of experience. My skills include Python, Java, and C++. I have worked on projects involving machine learning and web development.\\n I am an excellent team player.\",\n",
        "    \"Data Scientist with expertise in Python, R, and SQL. I have experience in statistical analysis, data visualization, and building predictive models. • Published a paper on a new machine learning algorithm.\",\n",
        "    \"A marketing specialist with a background in digital marketing, social media management, and content creation. Skills: SEO, SEM, Adobe Photoshop. I have a proven track record of increasing brand visibility.\"\n",
        "]}\n",
        "resumes_df = pd.DataFrame(data)\n",
        "\n",
        "# Q1. First 3 rows display chesi, noisy characters check chesthunna\n",
        "print(\"First 3 rows of the sample resumes:\")\n",
        "print(resumes_df.head(3))\n",
        "print(\"\\nChecking for noisy characters...\")\n",
        "print(f\"\\n present: {'\\n' in resumes_df['Resume_Text'].iloc[0]}\")\n",
        "print(f\"• present: {'•' in resumes_df['Resume_Text'].iloc[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# NLTK downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Add this line to download the missing resource\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsNhh9CmLLuf",
        "outputId": "66647dfa-bedb-4601-edbe-cdff54584b71"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. NLTK Preprocessing cheddam\n",
        "def preprocess_nltk(text):\n",
        "    # 1. Special characters and digits clean cheyyadam\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
        "    # 2. Tokenize cheyyadam\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # 3. Stop words remove cheyyadam\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
        "    # 4. Stemming cheyyadam\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "resumes_df['nltk_tokens'] = resumes_df['Resume_Text'].apply(preprocess_nltk)\n",
        "\n",
        "# Top 10 frequent stemmed words extract cheyyadam\n",
        "all_nltk_tokens = [token for sublist in resumes_df['nltk_tokens'] for token in sublist]\n",
        "fdist_nltk = nltk.FreqDist(all_nltk_tokens)\n",
        "top_10_nltk = fdist_nltk.most_common(10)\n",
        "\n",
        "print(\"\\n--- NLTK Preprocessing Results ---\")\n",
        "print(\"Processed Tokens for first resume:\")\n",
        "print(resumes_df['nltk_tokens'].iloc[0])\n",
        "print(\"\\nTop 10 frequent stemmed words:\")\n",
        "print(top_10_nltk)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_LTA0qdLOmS",
        "outputId": "5646a02c-ac6a-495f-995e-3c42e2375a47"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- NLTK Preprocessing Results ---\n",
            "Processed Tokens for first resume:\n",
            "['softwar', 'engin', 'year', 'experi', 'skill', 'includ', 'python', 'java', 'work', 'project', 'involv', 'machin', 'learn', 'web', 'develop', 'excel', 'team', 'player']\n",
            "\n",
            "Top 10 frequent stemmed words:\n",
            "[('experi', 2), ('skill', 2), ('python', 2), ('machin', 2), ('learn', 2), ('data', 2), ('market', 2), ('softwar', 1), ('engin', 1), ('year', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# spaCy model load cheyyadam\n",
        "# 'en_core_web_sm' model already install chesi undali. Leda \"python -m spacy download en_core_web_sm\" ani run cheyyali.\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Q3. spaCy Preprocessing cheddam\n",
        "def preprocess_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    # 1. Tokens ni lemmatize cheyyadam\n",
        "    # 2. Only alphabetic nouns and verbs ni filter cheyyadam\n",
        "    lemmas = [token.lemma_.lower() for token in doc if token.is_alpha and token.pos_ in ['NOUN', 'VERB']]\n",
        "    return lemmas\n",
        "\n",
        "resumes_df['spacy_lemmas'] = resumes_df['Resume_Text'].apply(preprocess_spacy)\n",
        "\n",
        "# Top 10 frequent lemmas extract cheyyadam\n",
        "all_spacy_lemmas = [lemma for sublist in resumes_df['spacy_lemmas'] for lemma in sublist]\n",
        "fdist_spacy = nltk.FreqDist(all_spacy_lemmas)\n",
        "top_10_spacy = fdist_spacy.most_common(10)\n",
        "\n",
        "print(\"\\n--- spaCy Preprocessing Results ---\")\n",
        "print(\"Processed Lemmas for first resume:\")\n",
        "print(resumes_df['spacy_lemmas'].iloc[0])\n",
        "print(\"\\nTop 10 frequent lemmas:\")\n",
        "print(top_10_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jXQjxSLQtJS",
        "outputId": "3c09f686-69be-4ab7-99d4-1b45f1d50431"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- spaCy Preprocessing Results ---\n",
            "Processed Lemmas for first resume:\n",
            "['software', 'engineer', 'year', 'experience', 'skill', 'include', 'work', 'project', 'involve', 'machine', 'learning', 'web', 'development', 'team', 'player']\n",
            "\n",
            "Top 10 frequent lemmas:\n",
            "[('experience', 2), ('skill', 2), ('machine', 2), ('have', 2), ('marketing', 2), ('software', 1), ('engineer', 1), ('year', 1), ('include', 1), ('work', 1)]\n"
          ]
        }
      ]
    }
  ]
}
